{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLProject3_Meniawy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "0ix2eyZWzCWa",
        "outputId": "a6eea1f4-5b5d-40c2-96cd-b8c0f4ffd355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# Specify the input shape to the first convolutional layer\n",
        "input_shape = (120, 160, 3)\n",
        "nClasses = 2\n",
        "def createModel():\n",
        "    model = Sequential()\n",
        "    # a convolution layer of 32 features of size 3x3 with relu activation and zero padding\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "    # a convolution layer of 32 features of size 3x3 with relu activation\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    # a batch normalization layer\n",
        "    model.add(BatchNormalization())\n",
        "    # maxpooling layer of filter size 2x2\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # a convolution layer of 64 features of size 3x3 with relu activation and zero padding\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    # a convolution layer of 64 features of size 3x3 with relu activation\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    # a batch normalization layer\n",
        "    model.add(BatchNormalization())\n",
        "    # maxpooling layer of filter size 2x2\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # a convolution layer of 64 features of size 3x3 with relu activation and zero padding\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    # a convolution layer of 64 features of size 3x3 with relu activation\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    # a batch normalization layer\n",
        "    model.add(BatchNormalization())\n",
        "    # maxpooling layer of filter size 2x2\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # a dropout layer of 50%\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    # flatten the output of the previous layer\n",
        "    model.add(Flatten())\n",
        "    # add a dense layer that outputs 512 units and apply relu activation\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    # a dropout layer of 50%\n",
        "    model.add(Dropout(0.5))\n",
        "    # add a dense layer with a softmax activation to classify the images\n",
        "    model.add(Dense(nClasses, activation='softmax'))\n",
        "     \n",
        "    return model\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "v9-HvJQyWd4D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1 = createModel()\n",
        "model1.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_R7DEGn2xDvj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Loading Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "t1ZbQJz9xHw6",
        "colab_type": "code",
        "outputId": "0f89af18-fcdb-401e-c35b-f98f10f0b1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BpN1FWZ67zip",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1- Use Keras’ ImageDataGenerator to read the modified dataset and continue training the provided model.\n",
        "#### 2- Use ImageDataGenerator’s parameters to create validation split (0.2) and to standardize the images."
      ]
    },
    {
      "metadata": {
        "id": "vyd9SwHe0Xk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Splitting training data as 0.8 for training and 0.2 for validation.\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "#         shear_range=0.2,\n",
        "#         zoom_range=0.2,\n",
        "#         horizontal_flip=True,\n",
        "        samplewise_std_normalization=True,\n",
        "        samplewise_center=True,\n",
        "        validation_split=0.2)\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    samplewise_std_normalization=True,\n",
        "    samplewise_center=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "27735ab8-9d9b-4481-e9c2-91a14a092fc0",
        "id": "-GBR32Pnf4IS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Reading training set\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/MLProject3/Data/train',\n",
        "        target_size=(120, 160),\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        subset=\"training\")\n",
        "#         color_mode=\"rgb\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3098 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rQVpkpW1gDiu",
        "colab_type": "code",
        "outputId": "11d3b8ae-53ea-4537-b021-eb67ce57ce03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Reading validation set\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/MLProject3/Data/train',\n",
        "        target_size=(120, 160),\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        subset=\"validation\"\n",
        "#         color_mode=\"rgb\"\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 774 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xZKhRIHvgF56",
        "colab_type": "code",
        "outputId": "a5bd6b85-0fac-46af-c862-30859acac6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Reading testing set\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/MLProject3/Data/test',\n",
        "        target_size=(120, 160),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "#         color_mode=\"rgb\"\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 860 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h65S5RgrCS7t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 4- Load the provided weights file and continue the training. Aim for validation accuracy of 92% at least."
      ]
    },
    {
      "metadata": {
        "id": "JNIVXLxoCSWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1.load_weights(\"/content/gdrive/My Drive/MLProject3/weights.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CVh-JyOcEp1n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 5- Use Keras’ checkpointing support to save the weights that achieve best validation performance. You will need to submit these."
      ]
    },
    {
      "metadata": {
        "id": "0kf0jgtaEy8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/callbacks/#modelcheckpoint\n",
        "weights_save_path = \"/content/gdrive/My Drive/MLProject3/best_weights.hdf5\"\n",
        "\n",
        "check_point = ModelCheckpoint(\n",
        "    weights_save_path, \n",
        "    save_best_only=True, \n",
        "    save_weights_only=True,\n",
        "    monitor='acc',\n",
        "    period=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xCiXy0o_VW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3- Use Keras’ model.fit_generator to train your model. Choose the appropriate steps_per_epoch and validation_steps value. These values are related to the batch size. \n",
        "\n",
        "#### Justify your choice.\n",
        "\n",
        "\n",
        "* The steps_per_epoch should typically be equal to the number of samples of your dataset divided by the batch size. So the number of data sample of the data set is train_generator.n and the batch size is train_generator.batch_size (32). Then I divided them by each other.\n",
        "\n",
        "* e.g. If a dataset has 1000 images and a batch size of 10, then the steps_per_epoch will be = 1000/10 = 100. Meaning that there will be 100 batches done per epoch (whole dataset).\n",
        "\n",
        "* Same goes for the validation_steps.\n",
        "\n",
        "* The validation_steps should typically be equal to the number of samples of your validation dataset divided by the batch size. So the number of data sample of the data set is validation_generator.n and the batch size is validation_generator.batch_size (32). Then I divided them by each other.\n",
        "\n",
        "* I have added a checkpoint to the list of callbacks to save the best weights."
      ]
    },
    {
      "metadata": {
        "id": "xcUZnCLL_C4K",
        "colab_type": "code",
        "outputId": "28f17ab6-6d9e-4e57-e70f-48c682ce247d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
        "STEP_SIZE_VALID = validation_generator.n // validation_generator.batch_size\n",
        "\n",
        "print(STEP_SIZE_TRAIN)\n",
        "print(len(train_generator))\n",
        "print(len(validation_generator))\n",
        "print(len(test_generator))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96\n",
            "97\n",
            "25\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nu1EYgT466NM",
        "colab_type": "code",
        "outputId": "0505f072-3151-4b09-8cc5-da1826cf924d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1059
        }
      },
      "cell_type": "code",
      "source": [
        "history = model1.fit_generator(\n",
        "            train_generator,\n",
        "            validation_data=validation_generator,\n",
        "            steps_per_epoch=STEP_SIZE_TRAIN, \n",
        "            verbose=1,\n",
        "            epochs=30,\n",
        "            validation_steps=STEP_SIZE_VALID, \n",
        "            callbacks=[check_point])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "96/96 [==============================] - 17s 179ms/step - loss: 0.5770 - acc: 0.8629 - val_loss: 0.6410 - val_acc: 0.8706\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 16s 162ms/step - loss: 0.3736 - acc: 0.8869 - val_loss: 0.7096 - val_acc: 0.8518\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 15s 161ms/step - loss: 0.3293 - acc: 0.9071 - val_loss: 1.0979 - val_acc: 0.8221\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 15s 161ms/step - loss: 0.4139 - acc: 0.9046 - val_loss: 1.4401 - val_acc: 0.8315\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 16s 168ms/step - loss: 0.3250 - acc: 0.9133 - val_loss: 0.6037 - val_acc: 0.8908\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 16s 166ms/step - loss: 0.2938 - acc: 0.9304 - val_loss: 5.4769 - val_acc: 0.6348\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 16s 165ms/step - loss: 0.2771 - acc: 0.9368 - val_loss: 0.9436 - val_acc: 0.8342\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 16s 165ms/step - loss: 0.2323 - acc: 0.9426 - val_loss: 0.6612 - val_acc: 0.8935\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 15s 161ms/step - loss: 0.2220 - acc: 0.9485 - val_loss: 1.6877 - val_acc: 0.8383\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 16s 163ms/step - loss: 0.1493 - acc: 0.9635 - val_loss: 0.7475 - val_acc: 0.8868\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 16s 164ms/step - loss: 0.2168 - acc: 0.9534 - val_loss: 0.6253 - val_acc: 0.9232\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 16s 164ms/step - loss: 0.1829 - acc: 0.9576 - val_loss: 1.1078 - val_acc: 0.8639\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 16s 162ms/step - loss: 0.1738 - acc: 0.9606 - val_loss: 0.6282 - val_acc: 0.9245\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 16s 163ms/step - loss: 0.1397 - acc: 0.9658 - val_loss: 0.6009 - val_acc: 0.9124\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 16s 170ms/step - loss: 0.1838 - acc: 0.9655 - val_loss: 1.2453 - val_acc: 0.8598\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 16s 164ms/step - loss: 0.1545 - acc: 0.9704 - val_loss: 0.8964 - val_acc: 0.8962\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 16s 164ms/step - loss: 0.1082 - acc: 0.9725 - val_loss: 0.6775 - val_acc: 0.8949\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 16s 162ms/step - loss: 0.1158 - acc: 0.9771 - val_loss: 1.3743 - val_acc: 0.8841\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 16s 165ms/step - loss: 0.1638 - acc: 0.9694 - val_loss: 1.4954 - val_acc: 0.8774\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 16s 162ms/step - loss: 0.0810 - acc: 0.9836 - val_loss: 1.0090 - val_acc: 0.9043\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 16s 163ms/step - loss: 0.1377 - acc: 0.9755 - val_loss: 0.7293 - val_acc: 0.9070\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 15s 160ms/step - loss: 0.0964 - acc: 0.9780 - val_loss: 0.9644 - val_acc: 0.9003\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 16s 165ms/step - loss: 0.0812 - acc: 0.9863 - val_loss: 1.0377 - val_acc: 0.8958\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 16s 161ms/step - loss: 0.1149 - acc: 0.9772 - val_loss: 0.9805 - val_acc: 0.8908\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 17s 172ms/step - loss: 0.0937 - acc: 0.9805 - val_loss: 0.6056 - val_acc: 0.9259\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 16s 165ms/step - loss: 0.1051 - acc: 0.9810 - val_loss: 0.8766 - val_acc: 0.9016\n",
            "Epoch 27/30\n",
            "96/96 [==============================] - 16s 162ms/step - loss: 0.0651 - acc: 0.9883 - val_loss: 1.2461 - val_acc: 0.8774\n",
            "Epoch 28/30\n",
            "96/96 [==============================] - 15s 160ms/step - loss: 0.0724 - acc: 0.9870 - val_loss: 0.7838 - val_acc: 0.9245\n",
            "Epoch 29/30\n",
            "96/96 [==============================] - 16s 165ms/step - loss: 0.0843 - acc: 0.9854 - val_loss: 1.6860 - val_acc: 0.8733\n",
            "Epoch 30/30\n",
            "96/96 [==============================] - 16s 165ms/step - loss: 0.0888 - acc: 0.9863 - val_loss: 1.0150 - val_acc: 0.8976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZBdZmK0N3Rx",
        "colab_type": "code",
        "outputId": "ee9239af-df4c-4880-9aa2-8eb99450a9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "em0vzLIHEvVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6- Evaluate your model on the test set provided."
      ]
    },
    {
      "metadata": {
        "id": "yDySsE2yEzgD",
        "colab_type": "code",
        "outputId": "0effa9cb-3860-47e7-9cf5-647487078eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
        "\n",
        "scores = model1.evaluate_generator(test_generator, verbose=1, steps=STEP_SIZE_TEST)\n",
        "\n",
        "# print(model1.metrics_names)\n",
        "# print(scores)\n",
        "print(\"\\n\")\n",
        "print(\"Validation Accurary: %.2f%%\" % max(history.history['val_acc']))\n",
        "print()\n",
        "print(\"Testing Accuracy: %.2f%%\" % scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 3s 119ms/step\n",
            "\n",
            "\n",
            "Validation Accurary: 0.93%\n",
            "\n",
            "Testing Accuracy: 0.96%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}